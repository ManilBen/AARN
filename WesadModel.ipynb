{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nf1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bss4LopoO224",
        "colab_type": "code",
        "outputId": "10c1e313-1a94-4339-ddce-587e1c5d5f97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcSEl6PKXIL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuYNNNecXOt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''  Upload dataset et delete labels 5,6,7 '''\n",
        "\n",
        "def upload_data(iggy):\n",
        "  x = pd.read_csv(\"drive/My Drive/inwr.csv\")\n",
        "  y = pd.read_csv(\"drive/My Drive/trwr.csv\")\n",
        "\n",
        "  x = x.iloc[:,1:].values\n",
        "  y = y.iloc[:,[1]].values\n",
        "  y = y.astype('int32')\n",
        "  x = x.astype('float32')\n",
        "  print(x)\n",
        "  if iggy == True :\n",
        "    ignore = [5,6,7]\n",
        "    for i in ignore :\n",
        "      fiv = np.where(y == i)\n",
        "      y = np.delete(y, fiv)\n",
        "      x = np.delete(x,fiv,axis=0)\n",
        "  y = y.reshape((y.shape[0],1))\n",
        "  return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNtmF8B6XQkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''    one hot encoding for labels   '''\n",
        "import keras\n",
        "\n",
        "def target_to_cat(y):\n",
        "  z = keras.utils.to_categorical(y)\n",
        "  return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxBqZaeuXT_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''    Splitting data '''\n",
        "\n",
        "from sklearn import preprocessing\n",
        "def data(x,z,sh):\n",
        "  \n",
        "  indices = np.arange(x.shape[0])\n",
        "  X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(x, z,indices, test_size=0.33, shuffle=sh)\n",
        "  \n",
        "  mm_scaler = preprocessing.MinMaxScaler()\n",
        "  X_train = mm_scaler.fit_transform(X_train)\n",
        "  X_test = mm_scaler.fit_transform(X_test)\n",
        "\n",
        "  \n",
        "  X_train, X_val, y_train, y_val, indices_train, indices_val = train_test_split(X_train, y_train,indices_train, test_size=0.3, shuffle=sh)\n",
        "  indices = np.append(indices_train, indices_test)\n",
        "  return X_train, X_val, y_train, y_val, X_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "521wEPvQAiWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''    Creation du modele :\n",
        "              nplx : number of neurons per layer \n",
        "              hdx : number of hidden layers\n",
        "       nplx will be multiplied by its hdx and add to the architecture\n",
        "       the final architecture is the concatenation of hd layers\n",
        "       Exemple : npl1 = 10 / npl2 = 32 / npl3 = 64 /\n",
        "                 hd1 = 2  /  hd2 = 1  / hd3 = 2\n",
        "       Architecture : 6-10-10-32-64-64-5\n",
        "'''\n",
        "\n",
        "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
        "\n",
        "def create_model(npl1, npl2,  npl3, \n",
        "                  hd1, hd2, hd3,\n",
        "                  activation,optimizer, input_shape=6, output_shape=5):\n",
        "  \n",
        "  \n",
        "  \n",
        "  model = Sequential()\n",
        "  # Add an input layer\n",
        "  first=True\n",
        "    \n",
        "  for i in range(hd1):\n",
        "    if first:\n",
        "      model.add(Dense(npl1, input_dim=input_shape, activation=activation))\n",
        "      first=False\n",
        "    else: \n",
        "      model.add(Dense(npl1, activation=activation))\n",
        "      \n",
        "  for i in range(hd2):\n",
        "    if first:\n",
        "      model.add(Dense(npl2, input_dim=input_shape, activation=activation))\n",
        "      first=False\n",
        "    else: \n",
        "      model.add(Dense(npl2, activation=activation))\n",
        "\n",
        "  for i in range(hd3):\n",
        "    if first:\n",
        "      model.add(Dense(npl3, input_dim=input_shape, activation=activation))\n",
        "      first=False\n",
        "    else: \n",
        "      model.add(Dense(npl3, activation=activation))    \n",
        "\n",
        "  model.add(Dense(5, activation=activation))\n",
        "  #compile model\n",
        "\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=\n",
        "  ['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khHp5-bCA9XE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping, TerminateOnNaN, ModelCheckpoint\n",
        "import time\n",
        "\n",
        "start = time.clock()\n",
        "print(\"Tps :\"+str((time.clock() - start)))\n",
        "\n",
        "x, y = upload_data(True)\n",
        "\n",
        "# one hot encoding\n",
        "y = target_to_cat(y)\n",
        "# split\n",
        "X_train, X_val, y_train, y_val, X_test, y_test = data(x,y,True)\n",
        "\n",
        "# Fonction d'activation\n",
        "act = 'sigmoid'\n",
        "\n",
        "# Fonctions d'apprentissage\n",
        "optimizer = ['adam','sgd']\n",
        "\n",
        "# nombre de couches\n",
        "nl1 = [0]\n",
        "nl2 = [2]\n",
        "nl3 = [1]\n",
        "\n",
        "# neuronnes par couche\n",
        "nn1=[0]\n",
        "nn2=[10]\n",
        "nn3=[10]\n",
        "\n",
        "\n",
        "\n",
        "bn1=0\n",
        "bn2=0 \n",
        "bn3=0 \n",
        "bl1=0\n",
        "bl2=0\n",
        "bl3 = 0 \n",
        "best_accuracy = 0.0\n",
        "stats = []\n",
        "best_val_loss = 1\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=40)\n",
        "tnan = TerminateOnNaN()\n",
        "\n",
        "for o in optimizer : \n",
        "  \n",
        "  for i in range(0,1):\n",
        "    l1 = random.choice(nl1)\n",
        "    l2 = random.choice(nl2)\n",
        "    l3 = random.choice(nl3)\n",
        "\n",
        "    n1 = random.choice(nn1)\n",
        "    n2 = random.choice(nn2)\n",
        "    n3 = random.choice(nn3)\n",
        "    \n",
        "\n",
        "    print(n1,n2,n3,l1,l2,l3,act,o)\n",
        "\n",
        "    save_clb = ModelCheckpoint(str(i)+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "    \n",
        "    model = create_model(n1,n2,n3,l1,l2,l3,act,o)\n",
        "\n",
        "    history = model.fit(X_train, y_train , validation_data=(X_val, y_val), batch_size=128 ,epochs=50, callbacks=[es,tnan,save_clb])\n",
        "    \n",
        "    stats.append([str(n1),str(n2),str(n3),str(l1),str(l2),str(l3),act,o,str(history.history['val_loss'][-1]),str(history.history['acc'][-1])])\n",
        "    \n",
        "    \n",
        "    # Courbe d'erreur\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "    # Courbe de precision\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "    # Evaluation de la performance du modele sur les données de test\n",
        "    print(model.evaluate(X_test,y_test))\n",
        "    \n",
        "    # Selection du meilleur modele\n",
        "    if (history.history['val_loss'][-1] < best_val_loss and history.history['acc'][-1] > best_accuracy):\n",
        "          bn1, bn2, bn3, bl1, bl2, bl3 = n1,n2,n3,l1,l2,l3\n",
        "          best_val_loss = history.history['val_loss'][-1]\n",
        "          best_accuracy = history.history['acc'][-1]\n",
        "          best_model = history\n",
        "          model.save('best_model.h5')\n",
        "      \n",
        "    end=time.clock()\n",
        "    hours, rem = divmod(end-start, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))   \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6441RCm5EbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('0.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02NGttlr96ok",
        "colab_type": "text"
      },
      "source": [
        "Searching for the best architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "438nLv-A-A0e",
        "colab_type": "text"
      },
      "source": [
        "Part 1 :\n",
        "      With & Without one hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgHZuKz_z2w2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
        "\n",
        "def create_model(npl1, npl2,  npl3, \n",
        "                  hd1, hd2, hd3,\n",
        "                  activation,optimizer, input_shape=6, output_shape=5):\n",
        "  \n",
        "  \n",
        "  \n",
        "  model = Sequential()\n",
        "  # Add an input layer\n",
        "  first=True\n",
        "    \n",
        "  for i in range(hd1):\n",
        "    if first:\n",
        "      model.add(Dense(npl1, input_dim=input_shape, activation=activation))\n",
        "      first=False\n",
        "    else: \n",
        "      model.add(Dense(npl1, activation=activation))\n",
        "      \n",
        "  for i in range(hd2):\n",
        "    if first:\n",
        "      model.add(Dense(npl2, input_dim=input_shape, activation=activation))\n",
        "      first=False\n",
        "    else: \n",
        "      model.add(Dense(npl2, activation=activation))\n",
        "\n",
        "  for i in range(hd3):\n",
        "    if first:\n",
        "      model.add(Dense(npl3, input_dim=input_shape, activation=activation))\n",
        "      first=False\n",
        "    else: \n",
        "      model.add(Dense(npl3, activation=activation))    \n",
        "\n",
        "  model.add(Dense(5, activation=activation))\n",
        "  #compile model\n",
        "\n",
        "  #es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=\n",
        "  ['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elhbooJoYMLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "x, y = upload_data(True)\n",
        "\n",
        "X_train, X_val, y_train, y_val, X_test, y_test = data(x,y,True)\n",
        "activation=['sigmoid']\n",
        "optimizer = ['adam']\n",
        "\n",
        "# numbers of layers\n",
        "nl1 = [1]\n",
        "nl2 = [0]\n",
        "nl3 = [0]\n",
        "\n",
        "# neurons in each layer\n",
        "nn1=[5]\n",
        "nn2=[5]\n",
        "nn3=[5]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "bn1=0\n",
        "bn2=0 \n",
        "bn3=0 \n",
        "bl1=0\n",
        "bl2=0\n",
        "bl3 = 0 \n",
        "best_accuracy = 0.0\n",
        "best_val_loss = 1\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "tnan = TerminateOnNaN()\n",
        "\n",
        "stats = []\n",
        "\n",
        "for i in range(0,2):\n",
        "  l1 = random.choice(nl1)\n",
        "  l2 = random.choice(nl2)\n",
        "  l3 = random.choice(nl3)\n",
        "\n",
        "  n1 = random.choice(nn1)\n",
        "  n2 = random.choice(nn2)\n",
        "  n3 = random.choice(nn3)\n",
        "\n",
        "  act = random.choice(activation)\n",
        "  \n",
        "  opt = random.choice(optimizer)\n",
        "  \n",
        "  print(n1,n2,n3,l1,l2,l3,act,opt)\n",
        "  model = create_model(n1,n2,n3,l1,l2,l3,act,opt)\n",
        "  \n",
        "  history = model.fit(X_train, y_train , validation_data=(X_val, y_val), epochs=8, callbacks=[es,tnan])\n",
        "  stats.append([str(n1),str(n2),str(n3),str(l1),str(l2),str(l3),act,opt,str(history.history['val_loss'][-1]),str(history.history['acc'][-1])])\n",
        "  \n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "    \n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  \n",
        "  \n",
        "  if (history.history['val_loss'][-1] < best_val_loss and history.history['acc'][-1]):\n",
        "        bn1, bn2, bn3, bl1, bl2, bl3 = n1,n2,n3,l1,l2,l3\n",
        "        best_val_loss = history.history['val_loss'][-1]\n",
        "        best_accuracy = history.history['acc'][-1]\n",
        "        best_model = history\n",
        "        model.save('best_model.h5')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EId77K4kKXoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savetxt(\"st1.csv\",stats, delimiter=\",\", fmt='%s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF3fkYOhZr6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('st1.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZuaGW5z-LIn",
        "colab_type": "text"
      },
      "source": [
        "Part 2 :\n",
        "      \n",
        "      Search for the number of layers and neurons per layer\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f58K3fx0HOJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from keras.callbacks import EarlyStopping, TerminateOnNaN, ModelCheckpoint\n",
        "\n",
        "x, y = upload_data(True)\n",
        "y = target_to_cat(y)\n",
        "\n",
        "X_train, X_val, y_train, y_val, X_test, y_test = data(x,y,True)\n",
        "activation=['sigmoid','softmax']\n",
        "optimizer = ['adam','adagrad','adadelta','sgd']\n",
        "# numbers of layers\n",
        "nl1 = [1,2,3]\n",
        "nl2 = [0,1,2,3]\n",
        "nl3 = [0,1,2,3]\n",
        "\n",
        "# neurons in each layer\n",
        "nn1=[5,10,20,50,80,100]\n",
        "nn2=[5,10,20,50,80,100]\n",
        "nn3=[5,10,20,50,80,100]\n",
        "\n",
        "stats = []\n",
        "\n",
        "\n",
        "bn1=0\n",
        "bn2=0 \n",
        "bn3=0 \n",
        "bl1=0\n",
        "bl2=0\n",
        "bl3 = 0 \n",
        "best_accuracy = 0.0\n",
        "best_val_loss = 1\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "tnan = TerminateOnNaN()\n",
        "\n",
        "for i in range(0,10):\n",
        "  l1 = random.choice(nl1)\n",
        "  l2 = random.choice(nl2)\n",
        "  l3 = random.choice(nl3)\n",
        "\n",
        "  n1 = random.choice(nn1)\n",
        "  n2 = random.choice(nn2)\n",
        "  n3 = random.choice(nn3)\n",
        "  \n",
        "  \n",
        "  act = random.choice(activation)\n",
        "  opt = random.choice(optimizer)\n",
        "  \n",
        "  print(n1,n2,n3,l1,l2,l3,opt,act)\n",
        "  \n",
        "  save_clb = ModelCheckpoint(str(i)+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  model = create_model(n1,n2,n3,l1,l2,l3,act,opt)\n",
        "  \n",
        "  history = model.fit(X_train, y_train , validation_data=(X_val, y_val), epochs=20, batch_size=512, callbacks=[es,tnan,save_clb])\n",
        "  stats.append([str(n1),str(n2),str(n3),str(l1),str(l2),str(l3),act,opt,str(history.history['val_loss'][-1]),str(history.history['acc'][-1])])\n",
        "  \n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "    \n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  \n",
        "  if (history.history['val_loss'][-1] < best_val_loss and history.history['acc'][-1]):\n",
        "        bn1, bn2, bn3, bl1, bl2, bl3 = n1,n2,n3,l1,l2,l3\n",
        "        best_val_loss = history.history['val_loss'][-1]\n",
        "        best_accuracy = history.history['acc'][-1]\n",
        "        best_model = history\n",
        "        model.save('best_model.h5')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzldTeIck6X0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savetxt(\"st2_3.csv\",stats, delimiter=\",\", fmt='%s')\n",
        "from google.colab import files\n",
        "files.download('st2_3.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kywd8prIIyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from keras.callbacks import EarlyStopping, TerminateOnNaN, ModelCheckpoint\n",
        "\n",
        "x, y = upload_data(True)\n",
        "y = target_to_cat(y)\n",
        "X_train, X_val, y_train, y_val, X_test, y_test = data(x,y,True)\n",
        "activation=['sigmoid','softmax']\n",
        "optimizer = ['adam','adagrad','adadelta']\n",
        "# numbers of layers\n",
        "nl1 = [1,2,3]\n",
        "nl2 = [0,1,2,3]\n",
        "nl3 = [0,1,2,3]\n",
        "\n",
        "# neurons in each layer\n",
        "nn1=[5,10,50,80,100]\n",
        "nn2=[5,10,50,80,100]\n",
        "nn3=[5,10,50,80,100]\n",
        "\n",
        "stats = []\n",
        "\n",
        "\n",
        "bn1=0\n",
        "bn2=0 \n",
        "bn3=0 \n",
        "bl1=0\n",
        "bl2=0\n",
        "bl3 = 0 \n",
        "best_accuracy = 0.0\n",
        "best_val_loss = 1\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "tnan = TerminateOnNaN()\n",
        "\n",
        "for i in range(0,10):\n",
        "  l1 = random.choice(nl1)\n",
        "  l2 = random.choice(nl2)\n",
        "  l3 = random.choice(nl3)\n",
        "\n",
        "  n1 = random.choice(nn1)\n",
        "  n2 = random.choice(nn2)\n",
        "  n3 = random.choice(nn3)\n",
        "  \n",
        "  \n",
        "  act = random.choice(activation)\n",
        "  opt = random.choice(optimizer)\n",
        "  \n",
        "  print(n1,n2,n3,l1,l2,l3,opt,act)\n",
        "  \n",
        "  save_clb = ModelCheckpoint(str(i)+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  model = create_model(n1,n2,n3,l1,l2,l3,act,opt)\n",
        "  \n",
        "  history = model.fit(X_train, y_train , validation_data=(X_val, y_val), epochs=50, batch_size=512, callbacks=[es,tnan,save_clb])\n",
        "  stats.append([str(n1),str(n2),str(n3),str(l1),str(l2),str(l3),act,opt,str(history.history['val_loss'][-1]),str(history.history['acc'][-1])])\n",
        "  \n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "    \n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  \n",
        "  if (history.history['val_loss'][-1] < best_val_loss and history.history['acc'][-1]):\n",
        "        bn1, bn2, bn3, bl1, bl2, bl3 = n1,n2,n3,l1,l2,l3\n",
        "        best_val_loss = history.history['val_loss'][-1]\n",
        "        best_accuracy = history.history['acc'][-1]\n",
        "        best_model = history\n",
        "        model.save('best_model.h5')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VluinPncIKFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savetxt(\"st2_3.csv\",stats, delimiter=\",\", fmt='%s')\n",
        "from google.colab import files\n",
        "files.download('st2_3.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZeWWHeG-jo0",
        "colab_type": "text"
      },
      "source": [
        "Partie 3 :\n",
        "      Archi avec 100 neurones par couche (sigmoid, adam)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40gs1qnaMpXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
        "\n",
        "def create_model(npl1, npl2,  npl3, \n",
        "                  hd1, hd2, hd3,\n",
        "                  activation,lr, input_shape=6, output_shape=5):\n",
        "  \n",
        "  \n",
        "  \n",
        "  model = Sequential()\n",
        "  # Add an input layer\n",
        "  first=True\n",
        "    \n",
        "  for i in range(hd1):\n",
        "    if first:\n",
        "      model.add(Dense(npl1, input_dim=input_shape, activation=activation))\n",
        "      first=False\n",
        "    else: \n",
        "      model.add(Dense(npl1, activation=activation))\n",
        "      \n",
        "  for i in range(hd2):\n",
        "    if first:\n",
        "      model.add(Dense(npl2, input_dim=input_shape, activation=activation))\n",
        "      first=False\n",
        "    else: \n",
        "      model.add(Dense(npl2, activation=activation))\n",
        "\n",
        "  for i in range(hd3):\n",
        "    if first:\n",
        "      model.add(Dense(npl3, input_dim=input_shape, activation=activation))\n",
        "      first=False\n",
        "    else: \n",
        "      model.add(Dense(npl3, activation=activation))    \n",
        "\n",
        "  model.add(Dense(5, activation=activation))\n",
        "  #compile model\n",
        "\n",
        "  #es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
        "  opt = keras.optimizers.Adam(lr=lr)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=\n",
        "  ['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGWr5MdDByCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from keras.callbacks import EarlyStopping, TerminateOnNaN, ModelCheckpoint\n",
        "\n",
        "x, y = upload_data(True)\n",
        "y = target_to_cat(y)\n",
        "X_train, X_val, y_train, y_val, X_test, y_test = data(x,y,True)\n",
        "act = 'sigmoid'\n",
        "optimizer = ['adam']\n",
        "learn=[0.1,0.01,0.001]\n",
        "# numbers of layers\n",
        "nl1 = [2]\n",
        "nl2 = [2,3]\n",
        "nl3 = [0]\n",
        "\n",
        "# neurons in each layer\n",
        "nn1=[100]\n",
        "nn2=[100]\n",
        "nn3=[0]\n",
        "\n",
        "bn1=0\n",
        "bn2=0 \n",
        "bn3=0 \n",
        "bl1=0\n",
        "bl2=0\n",
        "bl3 = 0 \n",
        "best_accuracy = 0.0\n",
        "stats = []\n",
        "best_val_loss = 1\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
        "tnan = TerminateOnNaN()\n",
        "\n",
        "for o in optimizer : \n",
        "  \n",
        "  for i in range(0,4):\n",
        "    l1 = random.choice(nl1)\n",
        "    l2 = random.choice(nl2)\n",
        "    l3 = random.choice(nl3)\n",
        "\n",
        "    n1 = random.choice(nn1)\n",
        "    n2 = random.choice(nn2)\n",
        "    n3 = random.choice(nn3)\n",
        "    lr = random.choice(learn)\n",
        "\n",
        "    print(n1,n2,l1,l2,l3,act,lr)\n",
        "\n",
        "    save_clb = ModelCheckpoint(str(i)+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "    model = create_model(n1,n2,n3,l1,l2,l3,act,lr)\n",
        "\n",
        "    history = model.fit(X_train, y_train , validation_data=(X_val, y_val), epochs=100, callbacks=[es,tnan,save_clb])\n",
        "    stats.append([str(n1),str(n2),str(n3),str(l1),str(l2),str(l3),act,opt,str(history.history['val_loss'][-1]),str(history.history['acc'][-1])])\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "    if (history.history['val_loss'][-1] < best_val_loss and history.history['acc'][-1]):\n",
        "          bn1, bn2, bn3, bl1, bl2, bl3 = n1,n2,n3,l1,l2,l3\n",
        "          best_val_loss = history.history['val_loss'][-1]\n",
        "          best_accuracy = history.history['acc'][-1]\n",
        "          best_model = history\n",
        "          model.save('best_model.h5')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX716furK_wO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savetxt(\"stp3.csv\",stats, delimiter=\",\", fmt='%s')\n",
        "from google.colab import files\n",
        "files.download('stp3.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_uSrf4FdtJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from keras.callbacks import EarlyStopping, TerminateOnNaN, ModelCheckpoint\n",
        "\n",
        "x, y = upload_data(True)\n",
        "y = target_to_cat(y)\n",
        "X_train, X_val, y_train, y_val, X_test, y_test = data(x,y,True)\n",
        "activation='sigmoid'\n",
        "optimizer = 'adam'\n",
        "# numbers of layers\n",
        "nl1 = [2]\n",
        "nl2 = [3]\n",
        "nl3 = [0]\n",
        "\n",
        "# neurons in each layer\n",
        "nn1=[100]\n",
        "nn2=[100]\n",
        "nn3=[0]\n",
        "\n",
        "\n",
        "bn1=0\n",
        "bn2=0 \n",
        "bn3=0 \n",
        "bl1=0\n",
        "bl2=0\n",
        "bl3 = 0 \n",
        "best_accuracy = 0.0\n",
        "best_val_loss = 1\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
        "tnan = TerminateOnNaN()\n",
        "\n",
        "for i in range(0,2):\n",
        "  l1 = random.choice(nl1)\n",
        "  l2 = random.choice(nl2)\n",
        "  l3 = random.choice(nl3)\n",
        "\n",
        "  n1 = random.choice(nn1)\n",
        "  n2 = random.choice(nn2)\n",
        "  n3 = random.choice(nn3)\n",
        "  \n",
        "  \n",
        "  print(n1,n2,l1,l2,l3)\n",
        "  \n",
        "  save_clb = ModelCheckpoint(str(i)+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  model = create_model(n1,n2,n3,l1,l2,l3,activation,optimizer)\n",
        "  \n",
        "  history = model.fit(X_train, y_train , validation_data=(X_val, y_val), epochs=100,batch_size=700, callbacks=[tnan,save_clb])\n",
        "  \n",
        "  if (history.history['val_loss'][-1] < best_val_loss and history.history['acc'][-1]):\n",
        "        bn1, bn2, bn3, bl1, bl2, bl3 = n1,n2,n3,l1,l2,l3\n",
        "        best_val_loss = history.history['val_loss'][-1]\n",
        "        best_accuracy = history.history['acc'][-1]\n",
        "        best_model = history\n",
        "        model.save('best_model.h5')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXBLJNISdMWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('best_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RScJ0cXGsPLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srte_8i5ssCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en-z8Whi_oe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBTfuVhwxl0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(best_model.history['loss'])\n",
        "plt.plot(best_model.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un5vm9A-s6A3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('best_model.h5') \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozCn4LeCdM_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "def create_model(npl1=1, npl2=1,  npl3=1, \n",
        "hd1=1000, hd2=500, hd3 = 200,\n",
        "activation = 'sigmoid',optimizer = 'adam', input_shape=8, output_shape=5):\n",
        "  \n",
        "  \n",
        "  \n",
        "  model = Sequential()\n",
        "  # Add an input layer\n",
        "  first=True\n",
        "    \n",
        "  for i in range(hd1):\n",
        "    if first:\n",
        "      model.add(Dense(npl1, input_dim=input_shape, activation=activation))\n",
        "      first=False\n",
        "    else: \n",
        "      model.add(Dense(npl1, activation=activation))\n",
        "      \n",
        "  for i in range(hd2):\n",
        "    if first:\n",
        "      model.add(Dense(npl2, input_dim=input_shape, activation=activation))\n",
        "      first=False\n",
        "    else: \n",
        "      model.add(Dense(npl2, activation=activation))\n",
        "\n",
        "  for i in range(hd3):\n",
        "    if first:\n",
        "      model.add(Dense(npl3, input_dim=input_shape, activation=activation))\n",
        "      first=False\n",
        "    else: \n",
        "      model.add(Dense(npl3, activation=activation))    \n",
        "\n",
        "  model.add(Dense(5, activation=activation))\n",
        "  #compile model\n",
        "\n",
        "  #es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=\n",
        "  ['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu-QjdVG1jwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = KerasClassifier(build_fn=create_model, epochs=1000, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7C2Cmc91lpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import PredefinedSplit\n",
        "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
        "\n",
        "x, y = upload_data(True)\n",
        "y = target_to_cat(y)\n",
        "\n",
        "X_train, X_val, y_train, y_val= data(x,y,True)\n",
        "\n",
        "#cv = PredefinedSplit(indices)\n",
        "\n",
        "# activation\n",
        "activation=['sigmoid']\n",
        "optimizer = ['adadelta','adam']\n",
        "# numbers of layers\n",
        "nl1 = [0,1,2,3]\n",
        "nl2 = [0,1,2,3]\n",
        "nl3 = [0,1,2,3]\n",
        "\n",
        "# neurons in each layer\n",
        "nn1=[8,16,32]\n",
        "nn2=[8,16,32]\n",
        "nn3=[8,16,32]\n",
        "\n",
        "batch_size = [128,256,512,1024]\n",
        "\n",
        "# dictionary summary\n",
        "param_grid = dict(\n",
        "                    npl1=nn1, npl2=nn2, npl3=nn3, hd1=nl1, hd2=nl2, hd3=nl3,\n",
        "                    activation=activation,optimizer=optimizer,batch_size=batch_size,\n",
        "                    input_shape=[x.shape[1]], output_shape = [y.shape[1]],\n",
        "                 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HATzoQh2gMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
        "\n",
        "\n",
        "grid = RandomizedSearchCV(estimator=model, cv=2, param_distributions=param_grid, \n",
        "                          verbose=20,  n_iter=5, n_jobs=-1)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
        "tnan = TerminateOnNaN()\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train,validation_data = (X_val, y_val),callbacks=[es,tnan])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpJvxBWJ3kpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_parameters = grid_result.best_params_\n",
        "best_accuracy = grid_result.best_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiyeAaJYFveK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
        "# activation\n",
        "activation=['relu', 'sigmoid']\n",
        "optimizer = ['adadelta','adam']\n",
        "# numbers of layers\n",
        "nl1 = [1,2,3]\n",
        "nl2 = [0,1,2,3]\n",
        "nl3 = [0,1,2,3]\n",
        "\n",
        "# neurons in each layer\n",
        "nn1=[10,50,80,100]\n",
        "nn2=[80,100,200]\n",
        "nn3=[50,20,10]\n",
        "\n",
        "epochs = [100,500,1000]\n",
        "\n",
        "batch_size = [200,512,1024]\n",
        "\n",
        "\n",
        "param_grid = dict(\n",
        "                    npl1=nl1, npl2=nl2, npl3=nl3, hd1=nn1, hd2=nn2, hd3=nn3,\n",
        "                    activation=activation,optimizer=optimizer,epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    input_shape=[x.shape[1]], output_shape = [y.shape[1]],\n",
        "                 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fm9nvT_FvvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid_search = GridSearchCV(estimator=model,\n",
        "                           param_grid = param_grid,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 3)\n",
        "grid_search = grid_search.fit(X_train, y_train, validation_data = (X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvA_r7amJfga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgE7e9A9Jfjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OaWGtS-sMuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "  \n",
        "x, y = upload_data(True)\n",
        "y = target_to_cat(y)\n",
        "\n",
        "X_train, X_val, y_train, y_val = data(x,y,True)\n",
        "model = KerasClassifier(build_fn=create_model)\n",
        "\n",
        "parameters = {'batch_size': [512, 20000, 120000],\n",
        "              'epochs': [500, 1000, 2000],\n",
        "              'optimizer': ['adam', 'rmsprop'],\n",
        "              }\n",
        "grid_search = GridSearchCV(estimator=model,\n",
        "                           param_grid = parameters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 10)\n",
        "grid_search = grid_search.fit(X_train, y_train, validation_data = (X_val, y_val),  callbacks=[es])\n",
        "best_parameters = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLG3g_aZqPqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model)\n",
        "optimizers = ['rmsprop', 'adam']\n",
        "epochs = np.array([500, 1000, 2000])\n",
        "batches = np.array([512, 20000, 120000])\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, nb_epoch=epochs, batch_size=batches)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "for params, mean_score, scores in grid_result.grid_scores_:\n",
        "    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRFvwFMAdsNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9XhqivAjysT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''   Fonction qui partitionne les données en 67 % entrainement et 33 % test   '''\n",
        "def splitting(x, z):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, z, test_size=0.33, shuffle=False)\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}